{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b3b425-cbcd-4ffc-ba91-b959ec81d563",
   "metadata": {},
   "source": [
    "## <center> École Polytechnique de Montréal <br> Département Génie Informatique et Génie Logiciel <br>  INF8460 – Traitement automatique de la langue naturelle <br> </center>\n",
    "## <center> TP1 INF8460 <br>  Automne 2021 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ee80b-4067-4557-ac0d-bb520798edfe",
   "metadata": {},
   "source": [
    "## 1. DESCRIPTION\n",
    "Dans ce TP, l’idée est d’effectuer de la recherche de passages de texte dans un corpus à partir d’une question en langue naturelle. Les questions et passages sont en anglais.\n",
    "\n",
    "Voici un exemple : <br>\n",
    "__Entrée : Question :__ What causes precipitation to fall?  \n",
    "\n",
    "__Solution - Trouver un passage qui contient la réponse à la question :__ In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under <mark> __gravity__ </mark>. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called “showers”. \n",
    "\n",
    "Ici la réponse est en gras dans le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f5728-6e71-477f-a195-7fde5828830c",
   "metadata": {},
   "source": [
    "## 2. LIBRARIES PERMISES\n",
    "- Jupyter notebook\n",
    "- NLTK\n",
    "- Numpy \n",
    "- Pandas\n",
    "- Sklearn\n",
    "- Pour toute autre librairie, demandez à votre chargé de laboratoire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563c521-2c21-42a3-810e-ab23604de07c",
   "metadata": {},
   "source": [
    "## 3. INFRASTRUCTURE\n",
    "\n",
    "- Vous avez accès aux GPU du local L-4818. Dans ce cas, vous devez utiliser le dossier temp (voir le tutoriel VirtualEnv.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e65690-9b7a-4e7f-bf0d-2e47b17c23bf",
   "metadata": {},
   "source": [
    "## 4. DESCRIPTION DES DONNEES\n",
    "\n",
    "Dans ce projet, vous utiliserez le jeu de données dans le répertoire _data_. Il est décomposé en données d’entrainement (train), de validation (dev) et de test (test). <br>\n",
    "\n",
    "Nous ne mettrons à votre disposition que les données d’entrainement et de validation. Les données de test ne contiennent pas le paragraphe de réponse et doivent être complétées avec les résultats de votre système.\n",
    "Nous vous fournissons un ensemble de données qui comprend un corpus (_corpus.csv_) qui contient tous les passages et leurs identificateurs (ID) et un jeu de données qui associe une question, un passage, et une réponse qui est directement extraite du passage. Notez que certains passages contiennent des balises HTML et qu’il vous faudra procéder à un prétraitement de ces passages pour les enlever. <br>\n",
    "Ce jeu de données est composé de trois sous-ensembles : \n",
    "- _Train_ : ensemble d’entraînement de la forme <QuestionID, QuestionText, PassageID, Réponse>. Le but est donc d’entrainer votre modèle à retrouver le passage qui contient la réponse à la question.\n",
    "- _Validation_ : De la même forme que le Train, il vous permet de valider votre entraînement et de tester les performances de certains modules.  \n",
    "- _Test_ : Un ensemble secret qui est utilisé pour évaluer votre système complet. Il est de la forme <QuestionID, Question>. Votre système doit trouver dans le corpus __corpus.csv__ le ou les passages les plus pertinents.\n",
    "\n",
    "Notez qu’il est possible de répondre aux requis du TP sans utiliser la réponse à la question. C’est à vous de choisir si vous utilisez la réponse ou non. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9e51e-b41f-4f8c-80a0-db978297f5cd",
   "metadata": {},
   "source": [
    "## 5. ETAPES DU TP \n",
    "A partir du notebook _inf8460_A21_TP1_ qui est distribué, vous devez réaliser les étapes suivantes. (Noter que les cellules dans le squelette sont là à titre informatif - il est fort probable que vous rajoutiez des sections au fur et à mesure de votre TP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc38d6-07ce-4985-9554-cfe1ac9bb395",
   "metadata": {},
   "source": [
    "Ci-dessous définir la constante _PATH_ qui doit être utilisée par votre code pour accéder aux fichiers. Il est attendu que pour la correction, le chargé de lab n'ait qu'à changer la valeur de _PATH_ pour le répertoire où se trouver les fichiers de datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b7f4fd1-63da-4757-a868-05587c68f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/\"\n",
    "N = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f9b01-430a-47d4-b400-e1310001ec4d",
   "metadata": {},
   "source": [
    "### 5.1. Pré-traitement (12 points)\n",
    "Les passages et questions de votre ensemble de données doivent d’abord être représentés et indexés pour ensuite pouvoir effectuer une recherche de passage pour répondre à une question. On vous demande donc d’implémenter une étape de pré-traitement des données.\n",
    "1) (_6 points_) Complétez les fonctions retournant les informations suivantes (une fonction par information, chaque fonction prenant en argument le corpus (passages, questions) composé d'une liste de phrases segmentées en jetons/tokens) :\n",
    "    1. Le nombre total de jetons (mots non distincts)\n",
    "    2. Le nombre total de mots distincts (les types qui constituent le vocabulaire)\n",
    "    3. Les N mots les plus fréquents du vocabulaire (N est un paramètre avec une valeur par défaut de 10) ainsi que leur fréquence\n",
    "    4. Le ratio jeton/type\n",
    "    5. Le nombre total de lemmes distincts\n",
    "    6. Le nombre total de racines (stems) distinctes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4a3661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dyela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dyela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dyela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import math \n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corpus = pandas.read_csv(PATH + \"corpus.csv\")\n",
    "data_questions = pandas.read_csv(PATH + \"train_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d40cb30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Norman dynasty had a major political, cult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The English name \"Normans\" comes from the Fren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In the course of the 10th century, the initial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Before Rollo's arrival, its populations did no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83322</th>\n",
       "      <td>83322</td>\n",
       "      <td>&lt;P&gt; Reevis ' first movie job was as a stunt ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83323</th>\n",
       "      <td>83323</td>\n",
       "      <td>&lt;P&gt; `` Up on Cripple Creek '' is the fifth son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83324</th>\n",
       "      <td>83324</td>\n",
       "      <td>&lt;P&gt; The fifth season of the American televisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83325</th>\n",
       "      <td>83325</td>\n",
       "      <td>&lt;P&gt; Footballer Danny Blanchflower turned down ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83326</th>\n",
       "      <td>83326</td>\n",
       "      <td>&lt;P&gt; Jacob Basil Anderson ( born 18 June 1990 )...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83327 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                          paragraph\n",
       "0          0  The Normans (Norman: Nourmands; French: Norman...\n",
       "1          1  The Norman dynasty had a major political, cult...\n",
       "2          2  The English name \"Normans\" comes from the Fren...\n",
       "3          3  In the course of the 10th century, the initial...\n",
       "4          4  Before Rollo's arrival, its populations did no...\n",
       "...      ...                                                ...\n",
       "83322  83322  <P> Reevis ' first movie job was as a stunt ri...\n",
       "83323  83323  <P> `` Up on Cripple Creek '' is the fifth son...\n",
       "83324  83324  <P> The fifth season of the American televisio...\n",
       "83325  83325  <P> Footballer Danny Blanchflower turned down ...\n",
       "83326  83326  <P> Jacob Basil Anderson ( born 18 June 1990 )...\n",
       "\n",
       "[83327 rows x 2 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0beb176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_example = [[\"banana\", \"bananas\", \"banana\", \"wolves\", \"wolf\"], [\"hello\", \"you\", \"are\", \"my\", \"friend\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796156c8-294b-4fed-af7a-17b0e7524d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(corpus):\n",
    "    counter = 0\n",
    "    for element in corpus :\n",
    "        counter += len(element)\n",
    "    return counter\n",
    "\n",
    "def count_types(corpus):\n",
    "    set_of_words = set()\n",
    "    for sentence in corpus :\n",
    "        for word in sentence : \n",
    "            set_of_words.add(word)\n",
    "    return len(set_of_words)\n",
    "\n",
    "def count_most_frequent_tokens(corpus, n):\n",
    "    tokens = defaultdict(lambda: 0)\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            tokens[word] += 1\n",
    "    \n",
    "    tokens = sorted(tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "    return tokens[0:n]\n",
    "\n",
    "def ratio_token_type(corpus):\n",
    "    return count_tokens(corpus) / count_types(corpus)\n",
    "\n",
    "def count_lemmas(corpus):\n",
    "    lemmzer = WordNetLemmatizer()\n",
    "    tokens = set()\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            tokens.add(lemmzer.lemmatize(word))\n",
    "    return len(tokens)\n",
    "\n",
    "def count_stems(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = set()\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            tokens.add(stemmer.stem(word))\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b0752-ceea-4d8f-82c7-8eb16c9223f0",
   "metadata": {},
   "source": [
    "2. (_1 point_) Ecrivez une fonction explore_corpus() qui fait appel à toutes les fonctions en 1) et imprime leur résultat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1817d29f-342b-41e2-aa46-1e79f77a6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_corpus(corpus):\n",
    "    print(\"Le nombre total de jetons: \", count_tokens(corpus))\n",
    "    print(\"Le nombre total de mots distincts: \", count_types(corpus))\n",
    "    print(\"Les N mots les plus fréquents du vocabulaires: \", count_most_frequent_tokens(corpus, N))\n",
    "    print(\"Le ratio jeton/type: \", ratio_token_type(corpus))\n",
    "    print(\"Le nombre total de racines (stems) distinctes: \", count_stems(corpus))\n",
    "    print(\"Le nombre total de lemmes distincts: \", count_lemmas(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e146f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre total de jetons:  10\n",
      "Le nombre total de mots distincts:  9\n",
      "Les N mots les plus fréquents du vocabulaires:  [('banana', 2), ('bananas', 1), ('wolves', 1), ('wolf', 1), ('hello', 1), ('you', 1), ('are', 1), ('my', 1), ('friend', 1)]\n",
      "Le ratio jeton/type:  1.1111111111111112\n",
      "Le nombre total de racines (stems) distinctes:  8\n",
      "Le nombre total de lemmes distincts:  7\n"
     ]
    }
   ],
   "source": [
    "explore_corpus(corpus_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46fffd-3fdc-4e36-bc37-e344b17697da",
   "metadata": {},
   "source": [
    "3. (_5 points_) Pour la suite du TP, vous devez effectuer le pré-traitement du corpus (questions, passages) en convertissant le texte en minuscules, en segmentant le texte, en supprimant les mots outils et en lemmatisant le texte. Chaque opération doit avoir sa fonction python si elle n’est pas déjà implantée dans la question 1) précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5a56ccb-19df-48b1-a844-5007988901b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    #Remove html's tag from a string 'text'\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text.lower().strip())\n",
    "\n",
    "def remove_stopword_and_punctuation(text):\n",
    "    return [word for word in text if not word in stopwords.words('english') and not word in string.punctuation]\n",
    "\n",
    "def lemmatize_tokens(text) :\n",
    "    lemmzer = WordNetLemmatizer()\n",
    "    return [lemmzer.lemmatize(word) for word in text]\n",
    "\n",
    "def text_preprocessor(text) :\n",
    "    text_clean = clean_html(text)\n",
    "    tokens = tokenize_text(text_clean)\n",
    "    clean_tokens = lemmatize_tokens(remove_stopword_and_punctuation(tokens))\n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "def preprocessing(df, column) :\n",
    "    #From an input dataframe and a given column which contains a string, add a new column with the text clean and tokenized\n",
    "    data = df[column].apply(text_preprocessor)\n",
    "    df[\"processed\"] = data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ec2a2-58e4-4a59-8f1a-da3b8c1c8a73",
   "metadata": {},
   "source": [
    "### 5.2. Représentation de questions et de passages (14 points)\n",
    "\n",
    "1. (_10 points_) En utilisant sklearn et à partir de votre corpus pré-traité, vous devez implanter un modèle M1 qui est de représenter chaque passage et question avec votre vocabulaire, en utilisant un modèle sac de mots des n-grammes (n=1) qu’ils contiennent et en pondérant ces éléments avec TF-IDF. Notez que les questions doivent aussi être inclues dans la construction du vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dea77a29-7e14-412c-a5dc-ad3222fce747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(corpus_df, questions_df) :\n",
    "    #Preprocess the corpus and the questions dataframe\n",
    "    corpus = preprocessing(corpus_df, \"paragraph\") \n",
    "    questions = preprocessing(questions_df, \"question\")\n",
    "    return corpus, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d60ca8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-f45d44dad5c9>:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"processed\"] = data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time in seconds: 26.675963401794434\n"
     ]
    }
   ],
   "source": [
    "#Preprocess on a smaller sample \n",
    "import time\n",
    "start_time = time.time()\n",
    "processed_corpus, processed_questions = preprocess_all(data_corpus[1000:2000], data_questions[0:1000])\n",
    "interval = time.time() - start_time\n",
    "print('Total time in seconds:', interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb7a2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_voc(processed_corpus, processed_questions, N) :\n",
    "    #From the processed question/corpus dataframes and an integer N, return a vocabulary of N words\n",
    "    all_words = processed_corpus[\"processed\"] + processed_questions[\"processed\"]\n",
    "    N_most_frequent_words = count_most_frequent_tokens(all_words,N)\n",
    "    return N_most_frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "740a9353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('problem', 159), ('norman', 89), ('time', 75), ('complexity', 67), ('algorithm', 51), (\"'s\", 50), ('machine', 46), ('turing', 39), ('class', 37), ('california', 36), ('southern', 33), ('many', 32), (\"''\", 31), ('``', 30), ('computational', 30), ('n', 29), ('instance', 27), ('input', 27), ('state', 26), ('one', 25), ('p', 24), ('normandy', 23), ('used', 23), ('known', 22), ('theory', 22), ('area', 21), ('language', 20), ('byzantine', 20), ('de', 20), ('np', 20)]\n"
     ]
    }
   ],
   "source": [
    "#Create and print the vocabulary\n",
    "vocabulary_freq = define_voc(processed_corpus, processed_questions, N)\n",
    "print(vocabulary_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d60f2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a concated dataframe with the column \"processed\" of questions and corpus dataframe\n",
    "proc_questions_and_corpus_df = pandas.DataFrame(pandas.concat([processed_corpus[\"processed\"], processed_questions[\"processed\"]]))\n",
    "proc_questions_and_corpus_df = proc_questions_and_corpus_df.set_index(pandas.Index(range(len(processed_corpus)+len(processed_questions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88b7aff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>norman norman nourmands french normands latin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>norman dynasty major political cultural milita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>english name `` norman '' come french word nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>course 10th century initially destructive incu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rollo 's arrival population differ picardy île...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>major concern intensive breeding program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>gene distribution decrease move away equator e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>event raymond e. brown believe toledot yeshu d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>transportation vehicle banned entering new delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>kind linguistics phonetics considered part</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             processed\n",
       "0    norman norman nourmands french normands latin ...\n",
       "1    norman dynasty major political cultural milita...\n",
       "2    english name `` norman '' come french word nor...\n",
       "3    course 10th century initially destructive incu...\n",
       "4    rollo 's arrival population differ picardy île...\n",
       "..                                                 ...\n",
       "195           major concern intensive breeding program\n",
       "196  gene distribution decrease move away equator e...\n",
       "197  event raymond e. brown believe toledot yeshu d...\n",
       "198   transportation vehicle banned entering new delhi\n",
       "199         kind linguistics phonetics considered part\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_questions_and_corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8cb00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_frequency_matrix(processed_df, vocabulary) :\n",
    "    #From the processed dataframe and the vocabulary, create the frequency matrix for each sentence and for each word\n",
    "    frequency_matrix = np.zeros((len(processed_df), len(vocabulary)))\n",
    "    for i in range(len(vocabulary)) :\n",
    "        voc_word = vocabulary[i][0]\n",
    "        for j in range(len(processed_df)) :\n",
    "            sentence = processed_df[\"processed\"][j]\n",
    "            for k in range(len(sentence)) :\n",
    "                if sentence[k] == voc_word :\n",
    "                    frequency_matrix[j][i] += 1\n",
    "                    \n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8ed2854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the frequency matrix with our sample\n",
    "frequency_matrix = construct_frequency_matrix(proc_questions_and_corpus_df, vocabulary_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "683f9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tf_idf(freq_matrix) :\n",
    "    #From a frequency matrix, compute the tfidf matrix\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    tfidf = transformer.fit_transform(freq_matrix)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03562ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the tfidf matrix with our sample\n",
    "tfidf = apply_tf_idf(frequency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15ce04a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200x30 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 553 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8209570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autre maniere de calculer tf-idf\n",
    "def new_tf_idf(processed_questions, processed_corpus, N) :\n",
    "    vectorizer = TfidfVectorizer(smooth_idf=False, max_features = N)\n",
    "    vectorizer.fit(pandas.concat([processed_questions[\"processed\"], processed_corpus[\"processed\"]], ignore_index = True))\n",
    "    tfidf_corpus = vectorizer.transform(processed_corpus[\"processed\"]) \n",
    "    print(vectorizer.get_feature_names())\n",
    "    return tfidf_corpus, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2b49ca8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', '100', '2006', '2007', '2008', '2010', '2013', '2014', '2015', 'according', 'album', 'along', 'also', 'although', 'america', 'american', 'among', 'announced', 'another', 'apple', 'area', 'around', 'art', 'artist', 'atticus', 'award', 'based', 'became', 'become', 'began', 'best', 'beyoncé', 'black', 'bond', 'book', 'british', 'building', 'called', 'capital', 'center', 'central', 'century', 'character', 'child', 'china', 'chinese', 'chopin', 'city', 'company', 'concert', 'control', 'could', 'country', 'county', 'court', 'day', 'death', 'development', 'due', 'dynasty', 'early', 'earthquake', 'east', 'emperor', 'empire', 'energy', 'established', 'european', 'even', 'example', 'family', 'film', 'first', 'five', 'following', 'force', 'form', 'fort', 'found', 'four', 'france', 'french', 'game', 'government', 'great', 'group', 'high', 'home', 'house', 'however', 'imperialism', 'including', 'ipod', 'island', 'khan', 'known', 'lama', 'large', 'largest', 'late', 'later', 'law', 'leader', 'led', 'lee', 'life', 'like', 'line', 'link', 'long', 'made', 'major', 'make', 'manhattan', 'many', 'may', 'medium', 'military', 'million', 'ming', 'mockingbird', 'mongol', 'much', 'music', 'name', 'national', 'new', 'north', 'novel', 'november', 'number', 'object', 'official', 'old', 'one', 'paris', 'park', 'part', 'people', 'performance', 'piano', 'place', 'play', 'polish', 'political', 'population', 'power', 'production', 'public', 'received', 'record', 'region', 'release', 'released', 'right', 'said', 'school', 'scout', 'second', 'set', 'several', 'since', 'single', 'solar', 'song', 'south', 'spectre', 'state', 'still', 'style', 'system', 'term', 'third', 'three', 'tibet', 'tibetan', 'time', 'title', 'two', 'united', 'university', 'use', 'used', 'using', 'version', 'video', 'war', 'warsaw', 'water', 'way', 'well', 'west', 'woman', 'work', 'world', 'would', 'writes', 'year', 'york']\n"
     ]
    }
   ],
   "source": [
    "tfidf, vectorizer = new_tf_idf(processed_questions, processed_corpus, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8767152-1f4e-4afc-ae4a-70bdac6002c7",
   "metadata": {},
   "source": [
    "2. (_4 points_) Expérimentez maintenant avec un modèle n-gramme (n=1,2) mélangeant les unigrammes et les bigrammes et pondéré avec TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d326ef8-19c2-42a8-b0cd-0956aa23c936",
   "metadata": {},
   "source": [
    "Pour M1 et M2, assurez-vous de réutiliser la même fonction avec comme paramètre les n-grammes à considérer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78b772-a289-4cc5-98a3-81ced0d06cc5",
   "metadata": {},
   "source": [
    "### 5.3. Ordonnancement des passages (10 points)\n",
    "Maintenant que vous avez une représentation de vos passages et questions, il faut être capable de déterminer quel passage sera le plus pertinent pour la question posée. Il vous faut donc retrouver un top-N (N=1,5,10 … ) de passages utiles pour répondre à la question. Ces passages devront être ordonnés du plus pertinent au moins pertinent. Idéalement le passage à la position 1 sera celui qui contient la réponse à la question.\n",
    "<br>\n",
    "<br>\n",
    "Vous devez écrire des fonctions pour évaluer la similarité entre la représentation de la question et celle de chaque passage et retourner les N passage les plus similaires où N est un paramètre. \n",
    "1. (_5 points_) En utilisant la distance euclidienne\n",
    "2. (_5 points_) En utilisant la distance cosinus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d4e2908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6cf655f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question, vectorizer) :\n",
    "    preprocessed_question = [text_preprocessor(question)]\n",
    "    return vectorizer.transform(preprocessed_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f2ae3f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_question = process_question(\"How long had John Paul II been the pope in 1979?\", vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5231e009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_question.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "63c98248-4da8-4e78-b75a-656891f270e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(tfidf_question, tfidf_corpus, data_corpus, N) :\n",
    "    distances = euclidean_distances(tfidf_question, tfidf_corpus)[0]\n",
    "    data_corpus[\"distance\"] = distances\n",
    "    sorted_data_corpus = data_corpus.sort_values(by='distance')\n",
    "    return sorted_data_corpus[0:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b6da885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-147-9d706b8e9914>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_corpus[\"distance\"] = distances\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1569    The map of earthquake intensity published by C...\n",
       "1004    The views of Ali Shariati, ideologue of the Ir...\n",
       "1768    New York City's commuter rail network is the l...\n",
       "1671    The Hudson River flows through the Hudson Vall...\n",
       "1565    In a United States Geological Survey (USGS) st...\n",
       "                              ...                        \n",
       "1617    As a result of the magnitude 7.9 earthquake an...\n",
       "1007    Another factor in the early 1990s that worked ...\n",
       "1119    The conflict is known by multiple names. In Br...\n",
       "1135    Jacques Legardeur de Saint-Pierre, who succeed...\n",
       "1613    The Red Cross Society of China flew 557 tents ...\n",
       "Name: paragraph, Length: 200, dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer(tfidf_question, tfidf, data_corpus[1000:2000], N)[\"paragraph\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095fb8eb-c62b-4808-af73-b1041b5c9b18",
   "metadata": {},
   "source": [
    "### 5.4. Évaluation (15 points)\n",
    "En utilisant votre ensemble de validation : <br>\n",
    "1. (_5 points_) Vous devez calculer la précision top-N (N=1,5,10, 50) de votre modèle M1 et M2 avec la distance euclidienne et cosinus et les afficher. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7cd6b-e5b3-48a4-8639-6f70d209a2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1458b14e-67a3-498c-9e6c-a13fec0e6677",
   "metadata": {},
   "source": [
    "2. (_5 points_) Pour chacun de ces modèles, générez une courbe de performance faisant varier le N (N=1, 5, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c40ca-6786-440f-bdd7-5948c2e9b864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a83efed6-eba2-4d9c-9329-f49e94c601d6",
   "metadata": {},
   "source": [
    "3. (_5 points_) A cette étape, vous devez produire un fichier _passage_submission_M1.csv_ et _passage_submission_M2.csv_ qui contient pour toutes les questions de l’ensemble de test le top-N des passages retournés par votre modèle M1 et M2 pour y répondre. C’est à vous de déterminer si vous utiliserez la distance euclidienne ou cosinus basé sur vos résultats d’évaluation sur l’ensemble de validation en 1) et 2). Le fichier doit respecter le format suivant pour chaque top_N(N=1,5,10,50) :  <QuestionID, PassageID1 ;… ;PassageIDN>. Le format est démontré dans _sample_passage_submission.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621634b9-162e-43f5-9aac-7c32d411213f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5443fae-3acb-442d-8086-18dfa744e832",
   "metadata": {},
   "source": [
    "### 5.5. Le plus (24 points)\n",
    "\n",
    "1. (_21 points_) Vous devez proposer un modèle M3 différent (basé sur l’apprentissage machine par exemple) afin de déterminer un score de pertinence d’un passage pour une question donnée et ordonner les passages. \n",
    "    - Faites une petite recherche sur l’état de l’art en consultant https://nlp.stanford.edu/IR-book/information-retrieval-book.html\n",
    "    - Vous êtes libres de proposer une autre métrique de poids, ou une autre façon d’ordonner les passages (exemple : méthodes de type _learning to rank_) et de partir de votre corpus initial ou de votre ordonnancement en M1/M2 (choisissez le meilleur) et de réordonnancer les passages obtenus par votre premier modèle.\n",
    "    - Expliquez votre modèle et son intérêt dans votre notebook. Le nombre de points obtenus dépendra de l’effort mis dans cette partie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011209f8-1330-4c8f-a9f0-3b7fa8262d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3f85d57-176a-4276-878d-2d9d5b5aea9b",
   "metadata": {},
   "source": [
    "2. (_2  point_) Vous devez ensuite afficher l’évaluation de votre modèle M3 tel que décrit dans la section 5.4 Evaluation en utilisant les mêmes fonctions. Notamment, vous devez comparer les performances de vos modèles M1, M2 et M3 sur l’ensemble de validation avec une courbe de performance faisant varier le N (N=1, 5, 10, …)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ee33d-a842-490c-ad8e-ab8c8ea3f53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "070749a9-c234-4340-a78c-ce63195c9532",
   "metadata": {},
   "source": [
    "3. (_1 point_) En utilisant votre modèle M3, vous devez produire un fichier passage_submission_M3.csv qui contient pour toutes les questions de l’ensemble de test le top-N des passages retournés par votre système pour y répondre. Le fichier doit respecter le format suivant pour chaque top_N (N=1,5,10,50) :  <QuestionID, PassageID1…PassageIDN>. _Le format est démontré dans sample_passage_submission.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7657f0-5336-462e-af3c-9ad4e5c30bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1735914-0bff-48b8-a160-5f56a8766c31",
   "metadata": {},
   "source": [
    "## LIVRABLES\n",
    "Vous devez remettre sur Moodle:\n",
    "1. _Le code_ : Un Jupyter notebook en Python qui contient le code implanté avec les librairies permises. Le code doit être exécutable sans erreur et accompagné des commentaires appropriés dans le notebook de manière à expliquer les différentes fonctions et étapes dans votre projet. Nous nous réservons le droit de demander une démonstration ou la preuve que vous avez effectué vous-mêmes les expériences décrites. _Attention, en aucun cas votre code ne doit avoir été copié d’une quelconque source_. Les critères de qualité tels que la lisibilité du code et des commentaires sont importants. Tout votre code et vos résultats doivent être exécutables et reproductibles ; \n",
    "2. Un fichier _requirements.txt_ doit indiquer toutes les librairies / données nécessaires ;\n",
    "3. Un lien _GoogleDrive_ ou similaire vers les modèles nécessaires pour exécuter votre notebook si approprié ;\n",
    "4. Les fichiers de soumission de données de test _passage_submission_M1.csv_ et _passage_submission_M2.csv_\n",
    "5. Un document _contributions.txt_ : Décrivez brièvement la contribution de chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail. En particulier, tous les membres du projet devraient participer à la conception du TP et participer activement à la réflexion et à l’implémentation du code.\n",
    "\n",
    "## EVALUATION \n",
    "Votre TP sera évalué selon les critères suivants :\n",
    "1. Exécution correcte du code\n",
    "2. Performance correcte des modèles\n",
    "3. Organisation du notebook\n",
    "4. Qualité du code (noms significatifs, structure, performance, gestion d’exception, etc.)\n",
    "5. Commentaires clairs et informatifs\n",
    "\n",
    "## CODE D’HONNEUR\n",
    "- Règle 1:  Le plagiat de code est bien évidemment interdit.\n",
    "- Règle 2: Vous êtes libres de discuter des idées et des détails de mise en œuvre avec d'autres équipes. Cependant, vous ne pouvez en aucun cas consulter le code d'une autre équipe INF8460, ou incorporer leur code dans votre TP.\n",
    "- Règle 3:  Vous ne pouvez pas partager votre code publiquement (par exemple, dans un dépôt GitHub public) tant que le cours n'est pas fini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a6349-a4bc-4630-bd36-6b929dcd19db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
