{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b3b425-cbcd-4ffc-ba91-b959ec81d563",
   "metadata": {},
   "source": [
    "## <center> École Polytechnique de Montréal <br> Département Génie Informatique et Génie Logiciel <br>  INF8460 – Traitement automatique de la langue naturelle <br> </center>\n",
    "## <center> TP1 INF8460 <br>  Automne 2021 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ee80b-4067-4557-ac0d-bb520798edfe",
   "metadata": {},
   "source": [
    "## 1. DESCRIPTION\n",
    "Dans ce TP, l’idée est d’effectuer de la recherche de passages de texte dans un corpus à partir d’une question en langue naturelle. Les questions et passages sont en anglais.\n",
    "\n",
    "Voici un exemple : <br>\n",
    "__Entrée : Question :__ What causes precipitation to fall?  \n",
    "\n",
    "__Solution - Trouver un passage qui contient la réponse à la question :__ In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under <mark> __gravity__ </mark>. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called “showers”. \n",
    "\n",
    "Ici la réponse est en gras dans le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f5728-6e71-477f-a195-7fde5828830c",
   "metadata": {},
   "source": [
    "## 2. LIBRARIES PERMISES\n",
    "- Jupyter notebook\n",
    "- NLTK\n",
    "- Numpy \n",
    "- Pandas\n",
    "- Sklearn\n",
    "- Pour toute autre librairie, demandez à votre chargé de laboratoire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563c521-2c21-42a3-810e-ab23604de07c",
   "metadata": {},
   "source": [
    "## 3. INFRASTRUCTURE\n",
    "\n",
    "- Vous avez accès aux GPU du local L-4818. Dans ce cas, vous devez utiliser le dossier temp (voir le tutoriel VirtualEnv.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e65690-9b7a-4e7f-bf0d-2e47b17c23bf",
   "metadata": {},
   "source": [
    "## 4. DESCRIPTION DES DONNEES\n",
    "\n",
    "Dans ce projet, vous utiliserez le jeu de données dans le répertoire _data_. Il est décomposé en données d’entrainement (train), de validation (dev) et de test (test). <br>\n",
    "\n",
    "Nous ne mettrons à votre disposition que les données d’entrainement et de validation. Les données de test ne contiennent pas le paragraphe de réponse et doivent être complétées avec les résultats de votre système.\n",
    "Nous vous fournissons un ensemble de données qui comprend un corpus (_corpus.csv_) qui contient tous les passages et leurs identificateurs (ID) et un jeu de données qui associe une question, un passage, et une réponse qui est directement extraite du passage. Notez que certains passages contiennent des balises HTML et qu’il vous faudra procéder à un prétraitement de ces passages pour les enlever. <br>\n",
    "Ce jeu de données est composé de trois sous-ensembles : \n",
    "- _Train_ : ensemble d’entraînement de la forme <QuestionID, QuestionText, PassageID, Réponse>. Le but est donc d’entrainer votre modèle à retrouver le passage qui contient la réponse à la question.\n",
    "- _Validation_ : De la même forme que le Train, il vous permet de valider votre entraînement et de tester les performances de certains modules.  \n",
    "- _Test_ : Un ensemble secret qui est utilisé pour évaluer votre système complet. Il est de la forme <QuestionID, Question>. Votre système doit trouver dans le corpus __corpus.csv__ le ou les passages les plus pertinents.\n",
    "\n",
    "Notez qu’il est possible de répondre aux requis du TP sans utiliser la réponse à la question. C’est à vous de choisir si vous utilisez la réponse ou non. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9e51e-b41f-4f8c-80a0-db978297f5cd",
   "metadata": {},
   "source": [
    "## 5. ETAPES DU TP \n",
    "A partir du notebook _inf8460_A21_TP1_ qui est distribué, vous devez réaliser les étapes suivantes. (Noter que les cellules dans le squelette sont là à titre informatif - il est fort probable que vous rajoutiez des sections au fur et à mesure de votre TP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc38d6-07ce-4985-9554-cfe1ac9bb395",
   "metadata": {},
   "source": [
    "Ci-dessous définir la constante _PATH_ qui doit être utilisée par votre code pour accéder aux fichiers. Il est attendu que pour la correction, le chargé de lab n'ait qu'à changer la valeur de _PATH_ pour le répertoire où se trouver les fichiers de datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7f4fd1-63da-4757-a868-05587c68f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"\"\n",
    "N = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f9b01-430a-47d4-b400-e1310001ec4d",
   "metadata": {},
   "source": [
    "### 5.1. Pré-traitement (12 points)\n",
    "Les passages et questions de votre ensemble de données doivent d’abord être représentés et indexés pour ensuite pouvoir effectuer une recherche de passage pour répondre à une question. On vous demande donc d’implémenter une étape de pré-traitement des données.\n",
    "1) (_6 points_) Complétez les fonctions retournant les informations suivantes (une fonction par information, chaque fonction prenant en argument le corpus (passages, questions) composé d'une liste de phrases segmentées en jetons/tokens) :\n",
    "    1. Le nombre total de jetons (mots non distincts)\n",
    "    2. Le nombre total de mots distincts (les types qui constituent le vocabulaire)\n",
    "    3. Les N mots les plus fréquents du vocabulaire (N est un paramètre avec une valeur par défaut de 10) ainsi que leur fréquence\n",
    "    4. Le ratio jeton/type\n",
    "    5. Le nombre total de lemmes distincts\n",
    "    6. Le nombre total de racines (stems) distinctes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a3661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import math \n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501860ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corpus = pandas.read_csv(PATH + \"corpus.csv\")\n",
    "data_questions = pandas.read_csv(PATH + \"train_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40cb30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Who leaders the sub-divisions of offices or di...</td>\n",
       "      <td>7544.0</td>\n",
       "      <td>deputy assistant directors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Besides using 3kV DC what other power type is ...</td>\n",
       "      <td>16461.0</td>\n",
       "      <td>25 kV 50 Hz AC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>How many other cities had populations larger t...</td>\n",
       "      <td>11041.0</td>\n",
       "      <td>Twenty-two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Did von Neumann rule hidden variable theories?</td>\n",
       "      <td>14002.0</td>\n",
       "      <td>von Neumann did not claim that his proof compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>What is the name of the book that has the laws...</td>\n",
       "      <td>6848.0</td>\n",
       "      <td>Torah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87296</th>\n",
       "      <td>106171</td>\n",
       "      <td>106171</td>\n",
       "      <td>who played annie's dad on 7th heaven</td>\n",
       "      <td>30072.0</td>\n",
       "      <td>Graham Jarvis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87297</th>\n",
       "      <td>106172</td>\n",
       "      <td>106172</td>\n",
       "      <td>where is the gallbladder situated in our body</td>\n",
       "      <td>76550.0</td>\n",
       "      <td>beneath the liver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87298</th>\n",
       "      <td>106173</td>\n",
       "      <td>106173</td>\n",
       "      <td>who believed that human ability is due to a co...</td>\n",
       "      <td>73560.0</td>\n",
       "      <td>Francis Galton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87299</th>\n",
       "      <td>106174</td>\n",
       "      <td>106174</td>\n",
       "      <td>when is bachelor in paradise airing in australia</td>\n",
       "      <td>37241.0</td>\n",
       "      <td>25 March 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87300</th>\n",
       "      <td>106175</td>\n",
       "      <td>106175</td>\n",
       "      <td>one decimal equal to how many square meter</td>\n",
       "      <td>69009.0</td>\n",
       "      <td>40.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87301 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0      id                                           question  \\\n",
       "0               0       0  Who leaders the sub-divisions of offices or di...   \n",
       "1               1       1  Besides using 3kV DC what other power type is ...   \n",
       "2               2       2  How many other cities had populations larger t...   \n",
       "3               3       3     Did von Neumann rule hidden variable theories?   \n",
       "4               5       5  What is the name of the book that has the laws...   \n",
       "...           ...     ...                                                ...   \n",
       "87296      106171  106171               who played annie's dad on 7th heaven   \n",
       "87297      106172  106172      where is the gallbladder situated in our body   \n",
       "87298      106173  106173  who believed that human ability is due to a co...   \n",
       "87299      106174  106174   when is bachelor in paradise airing in australia   \n",
       "87300      106175  106175         one decimal equal to how many square meter   \n",
       "\n",
       "       paragraph_id                                             answer  \n",
       "0            7544.0                         deputy assistant directors  \n",
       "1           16461.0                                     25 kV 50 Hz AC  \n",
       "2           11041.0                                         Twenty-two  \n",
       "3           14002.0  von Neumann did not claim that his proof compl...  \n",
       "4            6848.0                                              Torah  \n",
       "...             ...                                                ...  \n",
       "87296       30072.0                                      Graham Jarvis  \n",
       "87297       76550.0                                  beneath the liver  \n",
       "87298       73560.0                                     Francis Galton  \n",
       "87299       37241.0                                      25 March 2018  \n",
       "87300       69009.0                                              40.46  \n",
       "\n",
       "[87301 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0beb176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_example = [[\"banana\", \"bananas\", \"banana\", \"wolves\", \"wolf\"], [\"hello\", \"you\", \"are\", \"my\", \"friend\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796156c8-294b-4fed-af7a-17b0e7524d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(corpus):\n",
    "    counter = 0\n",
    "    for element in corpus :\n",
    "        counter += len(element)\n",
    "    return counter\n",
    "\n",
    "def count_types(corpus):\n",
    "    set_of_words = set()\n",
    "    for sentence in corpus :\n",
    "        for word in sentence : \n",
    "            set_of_words.add(word)\n",
    "    return len(set_of_words)\n",
    "\n",
    "def count_most_frequent_tokens(corpus, n):\n",
    "    tokens = defaultdict(lambda: 0)\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            tokens[word] += 1\n",
    "    \n",
    "    tokens = sorted(tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "    return tokens[0:n]\n",
    "\n",
    "def ratio_token_type(corpus):\n",
    "    return count_tokens(corpus) / count_types(corpus)\n",
    "\n",
    "def count_lemmas(corpus):\n",
    "    lemmzer = WordNetLemmatizer()\n",
    "    tokens = set()\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            tokens.add(lemmzer.lemmatize(word))\n",
    "    return len(tokens)\n",
    "\n",
    "def count_stems(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = set()\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            tokens.add(stemmer.stem(word))\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b0752-ceea-4d8f-82c7-8eb16c9223f0",
   "metadata": {},
   "source": [
    "2. (_1 point_) Ecrivez une fonction explore_corpus() qui fait appel à toutes les fonctions en 1) et imprime leur résultat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1817d29f-342b-41e2-aa46-1e79f77a6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_corpus(corpus):\n",
    "    print(\"Le nombre total de jetons: \", count_tokens(corpus))\n",
    "    print(\"Le nombre total de mots distincts: \", count_types(corpus))\n",
    "    print(\"Les N mots les plus fréquents du vocabulaires: \", count_most_frequent_tokens(corpus, N))\n",
    "    print(\"Le ratio jeton/type: \", ratio_token_type(corpus))\n",
    "    print(\"Le nombre total de racines (stems) distinctes: \", count_stems(corpus))\n",
    "    print(\"Le nombre total de lemmes distincts: \", count_lemmas(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e146f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre total de jetons:  10\n",
      "Le nombre total de mots distincts:  9\n",
      "Les N mots les plus fréquents du vocabulaires:  [('banana', 2), ('bananas', 1), ('wolves', 1), ('wolf', 1), ('hello', 1), ('you', 1), ('are', 1), ('my', 1), ('friend', 1)]\n",
      "Le ratio jeton/type:  1.1111111111111112\n",
      "Le nombre total de racines (stems) distinctes:  8\n",
      "Le nombre total de lemmes distincts:  7\n"
     ]
    }
   ],
   "source": [
    "explore_corpus(corpus_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46fffd-3fdc-4e36-bc37-e344b17697da",
   "metadata": {},
   "source": [
    "3. (_5 points_) Pour la suite du TP, vous devez effectuer le pré-traitement du corpus (questions, passages) en convertissant le texte en minuscules, en segmentant le texte, en supprimant les mots outils et en lemmatisant le texte. Chaque opération doit avoir sa fonction python si elle n’est pas déjà implantée dans la question 1) précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5a56ccb-19df-48b1-a844-5007988901b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    #Remove html's tag from a string 'text'\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text.lower().strip())\n",
    "\n",
    "def remove_stopword_and_punctuation(text):\n",
    "    return [word for word in text if not word in stopwords.words('english') and not word in string.punctuation]\n",
    "\n",
    "def lemmatize_tokens(text) :\n",
    "    lemmzer = WordNetLemmatizer()\n",
    "    return [lemmzer.lemmatize(word) for word in text]\n",
    "\n",
    "def preprocessing(df, column) :\n",
    "    #From an input dataframe and a given column which contains a string, add a new column with the text clean and tokenized\n",
    "    data = df[column].apply(clean_html).apply(tokenize_text).apply(remove_stopword_and_punctuation).apply(lemmatize_tokens)\n",
    "    df[\"processed\"] = list(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ec2a2-58e4-4a59-8f1a-da3b8c1c8a73",
   "metadata": {},
   "source": [
    "### 5.2. Représentation de questions et de passages (14 points)\n",
    "\n",
    "1. (_10 points_) En utilisant sklearn et à partir de votre corpus pré-traité, vous devez implanter un modèle M1 qui est de représenter chaque passage et question avec votre vocabulaire, en utilisant un modèle sac de mots des n-grammes (n=1) qu’ils contiennent et en pondérant ces éléments avec TF-IDF. Notez que les questions doivent aussi être inclues dans la construction du vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dea77a29-7e14-412c-a5dc-ad3222fce747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(corpus_df, questions_df) :\n",
    "    #Preprocess the corpus and the questions dataframe\n",
    "    corpus = preprocessing(corpus_df, \"paragraph\") \n",
    "    questions = preprocessing(questions_df, \"question\")\n",
    "    return corpus, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d60ca8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-175d238e153e>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"processed\"] = list(data)\n"
     ]
    }
   ],
   "source": [
    "#Preprocess on a smaller sample \n",
    "processed_corpus, processed_questions = preprocess_all(data_corpus[0:100], data_questions[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb7a2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_voc(processed_corpus, processed_questions, N) :\n",
    "    #From the processed question/corpus dataframes and an integer N, return a vocabulary of N words\n",
    "    all_words = processed_corpus[\"processed\"] + processed_questions[\"processed\"]\n",
    "    N_most_frequent_words = count_most_frequent_tokens(all_words,N)\n",
    "    return N_most_frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "740a9353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('problem', 159), ('norman', 89), ('time', 75), ('complexity', 67), ('algorithm', 51), (\"'s\", 50), ('machine', 46), ('turing', 39), ('class', 37), ('california', 36), ('southern', 33), ('many', 32), (\"''\", 31), ('``', 30), ('computational', 30), ('n', 29), ('instance', 27), ('input', 27), ('state', 26), ('one', 25), ('p', 24), ('normandy', 23), ('used', 23), ('known', 22), ('theory', 22), ('area', 21), ('language', 20), ('byzantine', 20), ('de', 20), ('np', 20)]\n"
     ]
    }
   ],
   "source": [
    "#Create and print the vocabulary\n",
    "vocabulary_freq = define_voc(processed_corpus, processed_questions, N)\n",
    "print(vocabulary_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d60f2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a concated dataframe with the column \"processed\" of questions and corpus dataframe\n",
    "proc_questions_and_corpus_df = pandas.DataFrame(pandas.concat([processed_corpus[\"processed\"], processed_questions[\"processed\"]]))\n",
    "proc_questions_and_corpus_df = proc_questions_and_corpus_df.set_index(pandas.Index(range(len(processed_corpus)+len(processed_questions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88b7aff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[norman, norman, nourmands, french, normands, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[norman, dynasty, major, political, cultural, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[english, name, ``, norman, '', come, french, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[course, 10th, century, initially, destructive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[rollo, 's, arrival, population, differ, picar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>[major, concern, intensive, breeding, program]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>[gene, distribution, decrease, move, away, equ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>[event, raymond, e., brown, believe, toledot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>[transportation, vehicle, banned, entering, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>[kind, linguistics, phonetics, considered, part]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             processed\n",
       "0    [norman, norman, nourmands, french, normands, ...\n",
       "1    [norman, dynasty, major, political, cultural, ...\n",
       "2    [english, name, ``, norman, '', come, french, ...\n",
       "3    [course, 10th, century, initially, destructive...\n",
       "4    [rollo, 's, arrival, population, differ, picar...\n",
       "..                                                 ...\n",
       "195     [major, concern, intensive, breeding, program]\n",
       "196  [gene, distribution, decrease, move, away, equ...\n",
       "197  [event, raymond, e., brown, believe, toledot, ...\n",
       "198  [transportation, vehicle, banned, entering, ne...\n",
       "199   [kind, linguistics, phonetics, considered, part]\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_questions_and_corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8cb00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_frequency_matrix(processed_df, vocabulary) :\n",
    "    #From the processed dataframe and the vocabulary, create the frequency matrix for each sentence and for each word\n",
    "    frequency_matrix = np.zeros((len(processed_df), len(vocabulary)))\n",
    "    for i in range(len(vocabulary)) :\n",
    "        voc_word = vocabulary[i][0]\n",
    "        for j in range(len(processed_df)) :\n",
    "            sentence = processed_df[\"processed\"][j]\n",
    "            for k in range(len(sentence)) :\n",
    "                if sentence[k] == voc_word :\n",
    "                    frequency_matrix[j][i] += 1\n",
    "                    \n",
    "    return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d8ed2854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the frequency matrix with our sample\n",
    "frequency_matrix = construct_frequency_matrix(proc_questions_and_corpus_df, vocabulary_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "683f9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tf_idf(freq_matrix) :\n",
    "    #From a frequency matrix, compute the tfidf matrix\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    tfidf = transformer.fit_transform(freq_matrix).toarray()\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03562ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.75145761, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.86303628, 0.        , ..., 0.15445984, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.20028243, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the tfidf matrix with our sample\n",
    "tfidf = apply_tf_idf(frequency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8767152-1f4e-4afc-ae4a-70bdac6002c7",
   "metadata": {},
   "source": [
    "2. (_4 points_) Expérimentez maintenant avec un modèle n-gramme (n=1,2) mélangeant les unigrammes et les bigrammes et pondéré avec TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ed1a2-09aa-4f60-9667-3d68e899b91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d326ef8-19c2-42a8-b0cd-0956aa23c936",
   "metadata": {},
   "source": [
    "Pour M1 et M2, assurez-vous de réutiliser la même fonction avec comme paramètre les n-grammes à considérer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78b772-a289-4cc5-98a3-81ced0d06cc5",
   "metadata": {},
   "source": [
    "### 5.3. Ordonnancement des passages (10 points)\n",
    "Maintenant que vous avez une représentation de vos passages et questions, il faut être capable de déterminer quel passage sera le plus pertinent pour la question posée. Il vous faut donc retrouver un top-N (N=1,5,10 … ) de passages utiles pour répondre à la question. Ces passages devront être ordonnés du plus pertinent au moins pertinent. Idéalement le passage à la position 1 sera celui qui contient la réponse à la question.\n",
    "<br>\n",
    "<br>\n",
    "Vous devez écrire des fonctions pour évaluer la similarité entre la représentation de la question et celle de chaque passage et retourner les N passage les plus similaires où N est un paramètre. \n",
    "1. (_5 points_) En utilisant la distance euclidienne\n",
    "2. (_5 points_) En utilisant la distance cosinus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c98248-4da8-4e78-b75a-656891f270e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "095fb8eb-c62b-4808-af73-b1041b5c9b18",
   "metadata": {},
   "source": [
    "### 5.4. Évaluation (15 points)\n",
    "En utilisant votre ensemble de validation : <br>\n",
    "1. (_5 points_) Vous devez calculer la précision top-N (N=1,5,10, 50) de votre modèle M1 et M2 avec la distance euclidienne et cosinus et les afficher. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7cd6b-e5b3-48a4-8639-6f70d209a2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1458b14e-67a3-498c-9e6c-a13fec0e6677",
   "metadata": {},
   "source": [
    "2. (_5 points_) Pour chacun de ces modèles, générez une courbe de performance faisant varier le N (N=1, 5, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c40ca-6786-440f-bdd7-5948c2e9b864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a83efed6-eba2-4d9c-9329-f49e94c601d6",
   "metadata": {},
   "source": [
    "3. (_5 points_) A cette étape, vous devez produire un fichier _passage_submission_M1.csv_ et _passage_submission_M2.csv_ qui contient pour toutes les questions de l’ensemble de test le top-N des passages retournés par votre modèle M1 et M2 pour y répondre. C’est à vous de déterminer si vous utiliserez la distance euclidienne ou cosinus basé sur vos résultats d’évaluation sur l’ensemble de validation en 1) et 2). Le fichier doit respecter le format suivant pour chaque top_N(N=1,5,10,50) :  <QuestionID, PassageID1 ;… ;PassageIDN>. Le format est démontré dans _sample_passage_submission.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621634b9-162e-43f5-9aac-7c32d411213f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5443fae-3acb-442d-8086-18dfa744e832",
   "metadata": {},
   "source": [
    "### 5.5. Le plus (24 points)\n",
    "\n",
    "1. (_21 points_) Vous devez proposer un modèle M3 différent (basé sur l’apprentissage machine par exemple) afin de déterminer un score de pertinence d’un passage pour une question donnée et ordonner les passages. \n",
    "    - Faites une petite recherche sur l’état de l’art en consultant https://nlp.stanford.edu/IR-book/information-retrieval-book.html\n",
    "    - Vous êtes libres de proposer une autre métrique de poids, ou une autre façon d’ordonner les passages (exemple : méthodes de type _learning to rank_) et de partir de votre corpus initial ou de votre ordonnancement en M1/M2 (choisissez le meilleur) et de réordonnancer les passages obtenus par votre premier modèle.\n",
    "    - Expliquez votre modèle et son intérêt dans votre notebook. Le nombre de points obtenus dépendra de l’effort mis dans cette partie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011209f8-1330-4c8f-a9f0-3b7fa8262d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3f85d57-176a-4276-878d-2d9d5b5aea9b",
   "metadata": {},
   "source": [
    "2. (_2  point_) Vous devez ensuite afficher l’évaluation de votre modèle M3 tel que décrit dans la section 5.4 Evaluation en utilisant les mêmes fonctions. Notamment, vous devez comparer les performances de vos modèles M1, M2 et M3 sur l’ensemble de validation avec une courbe de performance faisant varier le N (N=1, 5, 10, …)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ee33d-a842-490c-ad8e-ab8c8ea3f53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "070749a9-c234-4340-a78c-ce63195c9532",
   "metadata": {},
   "source": [
    "3. (_1 point_) En utilisant votre modèle M3, vous devez produire un fichier passage_submission_M3.csv qui contient pour toutes les questions de l’ensemble de test le top-N des passages retournés par votre système pour y répondre. Le fichier doit respecter le format suivant pour chaque top_N (N=1,5,10,50) :  <QuestionID, PassageID1…PassageIDN>. _Le format est démontré dans sample_passage_submission.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7657f0-5336-462e-af3c-9ad4e5c30bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1735914-0bff-48b8-a160-5f56a8766c31",
   "metadata": {},
   "source": [
    "## LIVRABLES\n",
    "Vous devez remettre sur Moodle:\n",
    "1. _Le code_ : Un Jupyter notebook en Python qui contient le code implanté avec les librairies permises. Le code doit être exécutable sans erreur et accompagné des commentaires appropriés dans le notebook de manière à expliquer les différentes fonctions et étapes dans votre projet. Nous nous réservons le droit de demander une démonstration ou la preuve que vous avez effectué vous-mêmes les expériences décrites. _Attention, en aucun cas votre code ne doit avoir été copié d’une quelconque source_. Les critères de qualité tels que la lisibilité du code et des commentaires sont importants. Tout votre code et vos résultats doivent être exécutables et reproductibles ; \n",
    "2. Un fichier _requirements.txt_ doit indiquer toutes les librairies / données nécessaires ;\n",
    "3. Un lien _GoogleDrive_ ou similaire vers les modèles nécessaires pour exécuter votre notebook si approprié ;\n",
    "4. Les fichiers de soumission de données de test _passage_submission_M1.csv_ et _passage_submission_M2.csv_\n",
    "5. Un document _contributions.txt_ : Décrivez brièvement la contribution de chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail. En particulier, tous les membres du projet devraient participer à la conception du TP et participer activement à la réflexion et à l’implémentation du code.\n",
    "\n",
    "## EVALUATION \n",
    "Votre TP sera évalué selon les critères suivants :\n",
    "1. Exécution correcte du code\n",
    "2. Performance correcte des modèles\n",
    "3. Organisation du notebook\n",
    "4. Qualité du code (noms significatifs, structure, performance, gestion d’exception, etc.)\n",
    "5. Commentaires clairs et informatifs\n",
    "\n",
    "## CODE D’HONNEUR\n",
    "- Règle 1:  Le plagiat de code est bien évidemment interdit.\n",
    "- Règle 2: Vous êtes libres de discuter des idées et des détails de mise en œuvre avec d'autres équipes. Cependant, vous ne pouvez en aucun cas consulter le code d'une autre équipe INF8460, ou incorporer leur code dans votre TP.\n",
    "- Règle 3:  Vous ne pouvez pas partager votre code publiquement (par exemple, dans un dépôt GitHub public) tant que le cours n'est pas fini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a6349-a4bc-4630-bd36-6b929dcd19db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
