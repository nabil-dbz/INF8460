{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b3b425-cbcd-4ffc-ba91-b959ec81d563",
   "metadata": {},
   "source": [
    "## <center> École Polytechnique de Montréal <br> Département Génie Informatique et Génie Logiciel <br>  INF8460 – Traitement automatique de la langue naturelle <br> </center>\n",
    "## <center> TP1 INF8460 <br>  Automne 2021 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ee80b-4067-4557-ac0d-bb520798edfe",
   "metadata": {},
   "source": [
    "## 1. DESCRIPTION\n",
    "Dans ce TP, l’idée est d’effectuer de la recherche de passages de texte dans un corpus à partir d’une question en langue naturelle. Les questions et passages sont en anglais.\n",
    "\n",
    "Voici un exemple : <br>\n",
    "__Entrée : Question :__ What causes precipitation to fall?  \n",
    "\n",
    "__Solution - Trouver un passage qui contient la réponse à la question :__ In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under <mark> __gravity__ </mark>. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called “showers”. \n",
    "\n",
    "Ici la réponse est en gras dans le texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f5728-6e71-477f-a195-7fde5828830c",
   "metadata": {},
   "source": [
    "## 2. LIBRARIES PERMISES\n",
    "- Jupyter notebook\n",
    "- NLTK\n",
    "- Numpy \n",
    "- Pandas\n",
    "- Sklearn\n",
    "- Pour toute autre librairie, demandez à votre chargé de laboratoire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563c521-2c21-42a3-810e-ab23604de07c",
   "metadata": {},
   "source": [
    "## 3. INFRASTRUCTURE\n",
    "\n",
    "- Vous avez accès aux GPU du local L-4818. Dans ce cas, vous devez utiliser le dossier temp (voir le tutoriel VirtualEnv.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e65690-9b7a-4e7f-bf0d-2e47b17c23bf",
   "metadata": {},
   "source": [
    "## 4. DESCRIPTION DES DONNEES\n",
    "\n",
    "Dans ce projet, vous utiliserez le jeu de données dans le répertoire _data_. Il est décomposé en données d’entrainement (train), de validation (dev) et de test (test). <br>\n",
    "\n",
    "Nous ne mettrons à votre disposition que les données d’entrainement et de validation. Les données de test ne contiennent pas le paragraphe de réponse et doivent être complétées avec les résultats de votre système.\n",
    "Nous vous fournissons un ensemble de données qui comprend un corpus (_corpus.csv_) qui contient tous les passages et leurs identificateurs (ID) et un jeu de données qui associe une question, un passage, et une réponse qui est directement extraite du passage. Notez que certains passages contiennent des balises HTML et qu’il vous faudra procéder à un prétraitement de ces passages pour les enlever. <br>\n",
    "Ce jeu de données est composé de trois sous-ensembles : \n",
    "- _Train_ : ensemble d’entraînement de la forme <QuestionID, QuestionText, PassageID, Réponse>. Le but est donc d’entrainer votre modèle à retrouver le passage qui contient la réponse à la question.\n",
    "- _Validation_ : De la même forme que le Train, il vous permet de valider votre entraînement et de tester les performances de certains modules.  \n",
    "- _Test_ : Un ensemble secret qui est utilisé pour évaluer votre système complet. Il est de la forme <QuestionID, Question>. Votre système doit trouver dans le corpus __corpus.csv__ le ou les passages les plus pertinents.\n",
    "\n",
    "Notez qu’il est possible de répondre aux requis du TP sans utiliser la réponse à la question. C’est à vous de choisir si vous utilisez la réponse ou non. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9e51e-b41f-4f8c-80a0-db978297f5cd",
   "metadata": {},
   "source": [
    "## 5. ETAPES DU TP \n",
    "A partir du notebook _inf8460_A21_TP1_ qui est distribué, vous devez réaliser les étapes suivantes. (Noter que les cellules dans le squelette sont là à titre informatif - il est fort probable que vous rajoutiez des sections au fur et à mesure de votre TP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc38d6-07ce-4985-9554-cfe1ac9bb395",
   "metadata": {},
   "source": [
    "Ci-dessous définir la constante _PATH_ qui doit être utilisée par votre code pour accéder aux fichiers. Il est attendu que pour la correction, le chargé de lab n'ait qu'à changer la valeur de _PATH_ pour le répertoire où se trouver les fichiers de datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7f4fd1-63da-4757-a868-05587c68f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ddfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taille du vocabulaire \n",
    "N = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc4e12cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ec24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture des dataframes utiles au TP\n",
    "data_corpus = pandas.read_csv(PATH + \"corpus.csv\")\n",
    "data_questions = pandas.read_csv(PATH + \"train_ids.csv\")\n",
    "data_validation = pandas.read_csv(PATH + \"val_ids.csv\")\n",
    "data_validation_reduced = pandas.read_csv(PATH + \"val_reduced.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f9b01-430a-47d4-b400-e1310001ec4d",
   "metadata": {},
   "source": [
    "### 5.1. Pré-traitement (12 points)\n",
    "Les passages et questions de votre ensemble de données doivent d’abord être représentés et indexés pour ensuite pouvoir effectuer une recherche de passage pour répondre à une question. On vous demande donc d’implémenter une étape de pré-traitement des données.\n",
    "1) (_6 points_) Complétez les fonctions retournant les informations suivantes (une fonction par information, chaque fonction prenant en argument le corpus (passages, questions) composé d'une liste de phrases segmentées en jetons/tokens) :\n",
    "    1. Le nombre total de jetons (mots non distincts)\n",
    "    2. Le nombre total de mots distincts (les types qui constituent le vocabulaire)\n",
    "    3. Les N mots les plus fréquents du vocabulaire (N est un paramètre avec une valeur par défaut de 10) ainsi que leur fréquence\n",
    "    4. Le ratio jeton/type\n",
    "    5. Le nombre total de lemmes distincts\n",
    "    6. Le nombre total de racines (stems) distinctes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0beb176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Un exemple simple pour vérifier l'efficacité des fonctions définies ci-dessous\n",
    "corpus_example = [[\"banana\", \"bananas\", \"banana\", \"wolves\", \"wolf\"], [\"hello\", \"you\", \"are\", \"my\", \"friend\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796156c8-294b-4fed-af7a-17b0e7524d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(corpus) :\n",
    "    #Compte le nombre de mots\n",
    "    counter = 0\n",
    "    for element in corpus :\n",
    "        counter += len(element)\n",
    "    return counter\n",
    "\n",
    "def count_types(corpus) :\n",
    "    #Compte le nombre de mots distincts\n",
    "    set_of_words = set()\n",
    "    for sentence in corpus :\n",
    "        for word in sentence : \n",
    "            set_of_words.add(word)\n",
    "    return len(set_of_words)\n",
    "\n",
    "def count_most_frequent_tokens(corpus, n):\n",
    "    #Repère les mots les plus fréquents et les retourne avec leurs fréquences d'apparitions associées\n",
    "    tokens = defaultdict(lambda: 0)\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            tokens[word] += 1\n",
    "    \n",
    "    tokens = sorted(tokens.items(), key=lambda x: x[1], reverse=True)\n",
    "    return tokens[0:n]\n",
    "\n",
    "def ratio_token_type(corpus):\n",
    "    return count_tokens(corpus) / count_types(corpus)\n",
    "\n",
    "def count_lemmas(corpus):\n",
    "    #Compte le nombre de mots distincts après lemmatisation\n",
    "    lemmzer = WordNetLemmatizer()\n",
    "    tokens = set()\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            tokens.add(lemmzer.lemmatize(word))\n",
    "    return len(tokens)\n",
    "\n",
    "def count_stems(corpus):\n",
    "    #Compte le nombre de mots distincts après stemmatization\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = set()\n",
    "    for text in corpus:\n",
    "        for word in text:\n",
    "            tokens.add(stemmer.stem(word))\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b0752-ceea-4d8f-82c7-8eb16c9223f0",
   "metadata": {},
   "source": [
    "2. (_1 point_) Ecrivez une fonction explore_corpus() qui fait appel à toutes les fonctions en 1) et imprime leur résultat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1817d29f-342b-41e2-aa46-1e79f77a6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_corpus(corpus):\n",
    "    print(\"Le nombre total de jetons: \", count_tokens(corpus))\n",
    "    print(\"Le nombre total de mots distincts: \", count_types(corpus))\n",
    "    print(\"Les N mots les plus fréquents du vocabulaires: \", count_most_frequent_tokens(corpus, N))\n",
    "    print(\"Le ratio jeton/type: \", round(ratio_token_type(corpus),2))\n",
    "    print(\"Le nombre total de racines (stems) distinctes: \", count_stems(corpus))\n",
    "    print(\"Le nombre total de lemmes distincts: \", count_lemmas(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e146f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre total de jetons:  10\n",
      "Le nombre total de mots distincts:  9\n",
      "Les N mots les plus fréquents du vocabulaires:  [('banana', 2), ('bananas', 1), ('wolves', 1), ('wolf', 1), ('hello', 1), ('you', 1), ('are', 1), ('my', 1), ('friend', 1)]\n",
      "Le ratio jeton/type:  1.11\n",
      "Le nombre total de racines (stems) distinctes:  8\n",
      "Le nombre total de lemmes distincts:  7\n"
     ]
    }
   ],
   "source": [
    "# Statistiques pour notre exemple de corpus\n",
    "explore_corpus(corpus_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea46fffd-3fdc-4e36-bc37-e344b17697da",
   "metadata": {},
   "source": [
    "3. (_5 points_) Pour la suite du TP, vous devez effectuer le pré-traitement du corpus (questions, passages) en convertissant le texte en minuscules, en segmentant le texte, en supprimant les mots outils et en lemmatisant le texte. Chaque opération doit avoir sa fonction python si elle n’est pas déjà implantée dans la question 1) précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5a56ccb-19df-48b1-a844-5007988901b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text) :\n",
    "    #Enlève les tags html d'un texte\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def remove_punctuation(text) :\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text.lower().strip())\n",
    "\n",
    "def remove_small_words(tokens, min_length) :\n",
    "    # From a list of tokens, remove the tokens which size are smaller than a given threshold min_length\n",
    "    for word in tokens :\n",
    "        if len(word) < min_length :\n",
    "            tokens.remove(word)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if not word in stopwords.words('english')]\n",
    "\n",
    "def lemmatize_tokens(tokens) :\n",
    "    lemmzer = WordNetLemmatizer()\n",
    "    return [lemmzer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def text_preprocessor(text) :\n",
    "    #Fonction qui trasnforme une phrase en vecteur de mots processés\n",
    "    text_clean = remove_punctuation(clean_html(text))\n",
    "    tokens = tokenize_text(text_clean)\n",
    "    clean_tokens = remove_small_words(lemmatize_tokens(remove_stopwords(tokens)),4)\n",
    "    return \" \".join(clean_tokens)\n",
    "\n",
    "def new_text_preprocessor(text) :\n",
    "    #Fonction qui trasnforme une phrase en vecteur de mots processés\n",
    "    text_clean = remove_punctuation(clean_html(text))\n",
    "    return text_clean\n",
    "\n",
    "def preprocessing(df, column) :\n",
    "    #Transforme une dataframe contenant du texte pour une colonne donnée en une nouvelle dataframe avec une colonne en plus\n",
    "    #de ce texte processé\n",
    "    data = df[column].apply(text_preprocessor)\n",
    "    data_copy = data.copy()\n",
    "    data_copy[\"processed\"] = data\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dfebf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(word_tokenize(doc)) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e4ed1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatokenizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ec2a2-58e4-4a59-8f1a-da3b8c1c8a73",
   "metadata": {},
   "source": [
    "### 5.2. Représentation de questions et de passages (14 points)\n",
    "\n",
    "1. (_10 points_) En utilisant sklearn et à partir de votre corpus pré-traité, vous devez implanter un modèle M1 qui est de représenter chaque passage et question avec votre vocabulaire, en utilisant un modèle sac de mots des n-grammes (n=1) qu’ils contiennent et en pondérant ces éléments avec TF-IDF. Notez que les questions doivent aussi être inclues dans la construction du vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba82456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index utilisé pour couper la taille de nos datasets d'entrainements de corpus et de questions\n",
    "index_to_cut = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f79ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pré-traitement du corpus, des questions d'entrainement et de l'ensemble de validation\n",
    "#processed_corpus = preprocessing(data_corpus[0:index_to_cut], \"paragraph\")\n",
    "#processed_questions = preprocessing(data_questions[0:index_to_cut], \"question\")\n",
    "#processed_validation = preprocessing(data_validation_reduced, \"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(processed_questions, processed_corpus, N, n_grams) :\n",
    "    #A partir des dataframes pré-traités de questions et de paragraphes, d'une taille de vocabulaire N et \n",
    "    #d'un couple n_grams, renvoie une matrice TF-IDF ainsi que son vectorizer associé\n",
    "    vectorizer = TfidfVectorizer(smooth_idf=False, max_features = N, ngram_range = n_grams)\n",
    "    vectorizer.fit(pandas.concat([processed_questions[\"processed\"], processed_corpus[\"processed\"]], ignore_index = True))\n",
    "    tfidf_corpus = vectorizer.transform(processed_corpus[\"processed\"]) \n",
    "    print(\"Vocabulary :\" + \"\\n\\n\", vectorizer.get_feature_names())\n",
    "    return tfidf_corpus, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_tfidf(data_questions, data_corpus, N, n_grams) : \n",
    "    vectorizer = TfidfVectorizer(smooth_idf=False, max_features = N, ngram_range = n_grams, \n",
    "                                 preprocessor = new_text_preprocessor, stop_words = \"english\",\n",
    "                                tokenizer = lemmatokenizer\n",
    "                                )\n",
    "    vectorizer.fit(pandas.concat([data_questions[\"question\"], data_corpus[\"paragraph\"]], ignore_index = True))\n",
    "    tfidf_corpus = vectorizer.transform(data_corpus[\"paragraph\"]) \n",
    "    print(\"Vocabulary :\" + \"\\n\\n\", vectorizer.get_feature_names())\n",
    "    return tfidf_corpus, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b49ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple pour le premier modèle avec affichage du vocabulaire\n",
    "tfidf_M1, vectorizer_M1 = new_tfidf(data_questions, data_corpus, N, (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8767152-1f4e-4afc-ae4a-70bdac6002c7",
   "metadata": {},
   "source": [
    "2. (_4 points_) Expérimentez maintenant avec un modèle n-gramme (n=1,2) mélangeant les unigrammes et les bigrammes et pondéré avec TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bed963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple pour le second modèle avec affichage du vocabulaire\n",
    "tfidf_M2, vectorizer_M2 = new_tfidf(data_questions, data_corpus, N, (1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d326ef8-19c2-42a8-b0cd-0956aa23c936",
   "metadata": {},
   "source": [
    "Pour M1 et M2, assurez-vous de réutiliser la même fonction avec comme paramètre les n-grammes à considérer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78b772-a289-4cc5-98a3-81ced0d06cc5",
   "metadata": {},
   "source": [
    "### 5.3. Ordonnancement des passages (10 points)\n",
    "Maintenant que vous avez une représentation de vos passages et questions, il faut être capable de déterminer quel passage sera le plus pertinent pour la question posée. Il vous faut donc retrouver un top-N (N=1,5,10 … ) de passages utiles pour répondre à la question. Ces passages devront être ordonnés du plus pertinent au moins pertinent. Idéalement le passage à la position 1 sera celui qui contient la réponse à la question.\n",
    "<br>\n",
    "<br>\n",
    "Vous devez écrire des fonctions pour évaluer la similarité entre la représentation de la question et celle de chaque passage et retourner les N passage les plus similaires où N est un paramètre. \n",
    "1. (_5 points_) En utilisant la distance euclidienne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ffa457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valeur top-N définie par défaut\n",
    "top_N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf655f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_question(question, vectorizer) :\n",
    "    #A partir d'une question au format string et d'un vectorizer, renvoie un vecteur après pré-traitement\n",
    "    #et vectorisation de la question\n",
    "    preprocessed_question = [text_preprocessor(question)]\n",
    "    return vectorizer.transform(preprocessed_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12074504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Un exemple d'application pour le premier modèle\n",
    "tfidf_question_M1 = process_question(\"Which section of the Rhine is most factories found?\", vectorizer_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03dc4051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2350 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_question_M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4e7239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Un exemple d'application pour le premier modèle\n",
    "tfidf_question_M2 = process_question(\"Which section of the Rhine is most factories found?\", vectorizer_M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d459587c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_question_M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63c98248-4da8-4e78-b75a-656891f270e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(tfidf_question, tfidf_corpus, data_corpus, distance, top_N) :\n",
    "    #A partir d'une question vectorisé, d'un corpus vectorisé, des données du corpus initial, d'une \n",
    "    #métrique de distance donnée et d'une valeur top-N, renvoie les top-N réponses à la question\n",
    "    distances = distance(tfidf_question, tfidf_corpus)[0]\n",
    "    data_corpus_copy = data_corpus.copy()\n",
    "    data_corpus_copy[\"distance\"] = distances\n",
    "    sorted_data_corpus = data_corpus_copy.sort_values(by='distance')\n",
    "    return sorted_data_corpus[0:top_N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6da885a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Robert Guiscard, an other Norman adventurer pr...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Norman dynasty had a major political, cult...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>Normans came into Scotland, building castles a...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>Southern California is also home to a large ho...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>In the visual arts, the Normans did not have t...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Subsequent to the Conquest, however, the March...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>The legendary religious zeal of the Normans wa...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Some Normans joined Turkish forces to aid in t...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>The Normans thereafter adopted the growing feu...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>Decision problems are one of the central objec...</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          paragraph  distance\n",
       "11  11  Robert Guiscard, an other Norman adventurer pr...  1.414214\n",
       "1    1  The Norman dynasty had a major political, cult...  1.414214\n",
       "20  20  Normans came into Scotland, building castles a...  1.414214\n",
       "94  94  Southern California is also home to a large ho...  1.414214\n",
       "33  33  In the visual arts, the Normans did not have t...  1.414214\n",
       "..  ..                                                ...       ...\n",
       "22  22  Subsequent to the Conquest, however, the March...  1.414214\n",
       "23  23  The legendary religious zeal of the Normans wa...  1.414214\n",
       "9    9  Some Normans joined Turkish forces to aid in t...  1.414214\n",
       "6    6  The Normans thereafter adopted the growing feu...  1.414214\n",
       "45  45  Decision problems are one of the central objec...  1.414214\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exemple de réponses pour le premier modèle et la distance euclidienne\n",
    "question_answer(tfidf_question_M1, tfidf_M1, data_corpus[0:index_to_cut], euclidean_distances, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d7ce67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>But bounding the computation time above by som...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Soon after the Normans began to enter Italy, t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Even before the Norman Conquest of England, th...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>In Britain, Norman art primarily survives as s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In the course of the 10th century, the initial...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>The motion picture, television, and music indu...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>A computational problem can be viewed as an in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>In the visual arts, the Normans did not have t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>Many complexity classes are defined using the ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>In 1900, the Los Angeles Times defined souther...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          paragraph  distance\n",
       "63  63  But bounding the computation time above by som...       1.0\n",
       "7    7  Soon after the Normans began to enter Italy, t...       1.0\n",
       "21  21  Even before the Norman Conquest of England, th...       1.0\n",
       "36  36  In Britain, Norman art primarily survives as s...       1.0\n",
       "3    3  In the course of the 10th century, the initial...       1.0\n",
       "93  93  The motion picture, television, and music indu...       1.0\n",
       "42  42  A computational problem can be viewed as an in...       1.0\n",
       "33  33  In the visual arts, the Normans did not have t...       1.0\n",
       "68  68  Many complexity classes are defined using the ...       1.0\n",
       "99  99  In 1900, the Los Angeles Times defined souther...       1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exemple de réponses pour le second modèle et la distance euclidienne\n",
    "question_answer(tfidf_question_M2, tfidf_M2, data_corpus[0:index_to_cut], euclidean_distances, top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c4594",
   "metadata": {},
   "source": [
    "2. (_5 points_) En utilisant la distance cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4143ee62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>The complexity class P is often seen as a math...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>If a problem X is in C and hard for C, then X ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>This motivates the concept of a problem being ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>The most commonly used reduction is a polynomi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>Many complexity classes are defined using the ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>The time and space hierarchy theorems form the...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>For the complexity classes defined in this way...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>Other important complexity classes include BPP...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>Many important complexity classes can be defin...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          paragraph  distance\n",
       "0    0  The Normans (Norman: Nourmands; French: Norman...       1.0\n",
       "72  72  The complexity class P is often seen as a math...       1.0\n",
       "71  71  If a problem X is in C and hard for C, then X ...       1.0\n",
       "70  70  This motivates the concept of a problem being ...       1.0\n",
       "69  69  The most commonly used reduction is a polynomi...       1.0\n",
       "68  68  Many complexity classes are defined using the ...       1.0\n",
       "67  67  The time and space hierarchy theorems form the...       1.0\n",
       "66  66  For the complexity classes defined in this way...       1.0\n",
       "65  65  Other important complexity classes include BPP...       1.0\n",
       "64  64  Many important complexity classes can be defin...       1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exemple de réponses pour le premier modèle et la distance cosinus\n",
    "question_answer(tfidf_question_M1, tfidf_M1, data_corpus[0:index_to_cut], cosine_distances, top_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecc2f577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>The complexity class P is often seen as a math...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>If a problem X is in C and hard for C, then X ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>This motivates the concept of a problem being ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>The most commonly used reduction is a polynomi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>Many complexity classes are defined using the ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>The time and space hierarchy theorems form the...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>For the complexity classes defined in this way...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>Other important complexity classes include BPP...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>Many important complexity classes can be defin...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          paragraph  distance\n",
       "0    0  The Normans (Norman: Nourmands; French: Norman...       1.0\n",
       "72  72  The complexity class P is often seen as a math...       1.0\n",
       "71  71  If a problem X is in C and hard for C, then X ...       1.0\n",
       "70  70  This motivates the concept of a problem being ...       1.0\n",
       "69  69  The most commonly used reduction is a polynomi...       1.0\n",
       "68  68  Many complexity classes are defined using the ...       1.0\n",
       "67  67  The time and space hierarchy theorems form the...       1.0\n",
       "66  66  For the complexity classes defined in this way...       1.0\n",
       "65  65  Other important complexity classes include BPP...       1.0\n",
       "64  64  Many important complexity classes can be defin...       1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exemple de réponses pour le second modèle et la distance cosinus\n",
    "question_answer(tfidf_question_M2, tfidf_M2, data_corpus[0:index_to_cut], cosine_distances, top_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095fb8eb-c62b-4808-af73-b1041b5c9b18",
   "metadata": {},
   "source": [
    "### 5.4. Évaluation (15 points)\n",
    "En utilisant votre ensemble de validation : <br>\n",
    "1. (_5 points_) Vous devez calculer la précision top-N (N=1,5,10, 50) de votre modèle M1 et M2 avec la distance euclidienne et cosinus et les afficher. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50c7cd6b-e5b3-48a4-8639-6f70d209a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(data_val, data_corpus, vectorizer, distance, tfidf, top_N, question_answer_func) :\n",
    "    #A partir des datasets de validation et du corpus, d'une matrice tf-idf et de son vectorizer associé, d'une métrique de\n",
    "    #distance et d'une valeur top-N, renvoie la précision top-N pour l'ensemble des questions\n",
    "    count_good_answers = 0\n",
    "    for i in range(len(data_val)) :\n",
    "        raw_question = data_val[\"question\"][i]\n",
    "        tfidf_question = process_question(raw_question, vectorizer)\n",
    "        paragraph_id_answer = data_val[\"paragraph_id\"][i]\n",
    "        indexes_top_N = question_answer_func(tfidf_question, tfidf, data_corpus, distance, top_N)[\"id\"]\n",
    "        if paragraph_id_answer in(indexes_top_N) :\n",
    "            count_good_answers += 1\n",
    "    return round(count_good_answers/len(data_val)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c543bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcul de la précision pour le premier modèle et la distance euclidienne \n",
    "#precision_euclidean_M1 = compute_precision(data_validation, data_corpus[0:index_to_cut], vectorizer_M1, euclidean_distances, tfidf_M1, top_N, question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d118daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcul de la précision pour le second modèle et la distance euclidienne \n",
    "#precision_euclidean_M2 = compute_precision(data_validation, data_corpus[0:index_to_cut], vectorizer_M2, euclidean_distances, tfidf_M2, top_N, question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95e1e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcul de la précision pour le premier modèle et la distance cosinus \n",
    "#precision_cosine_M1 = compute_precision(data_validation, data_corpus[0:index_to_cut], vectorizer_M1, cosine_distances, tfidf_M1, top_N, question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f482d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcul de la précision pour le second modèle et la distance cosinus \n",
    "#precision_cosine_M2 = compute_precision(data_validation, data_corpus[0:index_to_cut], vectorizer_M2, cosine_distances, tfidf_M2, top_N, question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92c37aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the first model and the euclidean distance, the precision is about 0.05 %\n",
      "For the second model and the euclidean distance, the precision is about 0.05 %\n",
      "For the first model and the cosine distance, the precision is about 0.05 %\n",
      "For the second model and the cosine distance, the precision is about 0.05 %\n"
     ]
    }
   ],
   "source": [
    "print(\"For the first model and the euclidean distance, the precision is about\", precision_euclidean_M1, \"%\")\n",
    "print(\"For the second model and the euclidean distance, the precision is about\", precision_euclidean_M2, \"%\")\n",
    "print(\"For the first model and the cosine distance, the precision is about\", precision_cosine_M1, \"%\")\n",
    "print(\"For the second model and the cosine distance, the precision is about\", precision_cosine_M2, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458b14e-67a3-498c-9e6c-a13fec0e6677",
   "metadata": {},
   "source": [
    "2. (_5 points_) Pour chacun de ces modèles, générez une courbe de performance faisant varier le N (N=1, 5, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c2c40ca-6786-440f-bdd7-5948c2e9b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Liste des top-N \n",
    "tops_N = [1, 5, 10, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "889c9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(data_val, data_corpus, vectorizer, distance, tfidf, tops_N, model_name, distance_name, question_answer_func) :\n",
    "    #Fonction qui trace l'évolution de la précision en fonction de la valeur top-N pour un modèle et une distance donnée \n",
    "    precisions = []\n",
    "    for current_N in tops_N :\n",
    "        print(\"Current N :\", current_N)\n",
    "        precision = compute_precision(data_val, data_corpus, vectorizer, distance, tfidf, current_N, question_answer_func)\n",
    "        precisions.append(precision)\n",
    "    plt.plot(tops_N, precisions, label = \"Distance \" + distance_name)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ac569e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current N : 1\n",
      "Current N : 5\n",
      "Current N : 10\n",
      "Current N : 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SeekableUnicodeStreamReader.__del__ at 0x0000020C0E9AE940>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\simon\\anaconda3\\lib\\site-packages\\nltk\\data.py\", line 1160, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\simon\\anaconda3\\lib\\site-packages\\nltk\\data.py\", line 1189, in close\n",
      "    self.stream.close()\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-b72f730a5b6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Trace les deux courbes pour le modèle M1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mgenerate_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_corpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindex_to_cut\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer_M1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_M1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtops_N\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"M1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"euclidienne\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_answer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mgenerate_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_corpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindex_to_cut\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer_M1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosine_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_M1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtops_N\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"M1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cosinus\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_answer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-6ee46c92fc27>\u001b[0m in \u001b[0;36mgenerate_plot\u001b[1;34m(data_val, data_corpus, vectorizer, distance, tfidf, tops_N, model_name, distance_name, question_answer_func)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcurrent_N\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtops_N\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Current N :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_N\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_precision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_N\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion_answer_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprecisions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtops_N\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Distance \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdistance_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-25fc7c00fc24>\u001b[0m in \u001b[0;36mcompute_precision\u001b[1;34m(data_val, data_corpus, vectorizer, distance, tfidf, top_N, question_answer_func)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mraw_question\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"question\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtfidf_question\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_question\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_question\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mparagraph_id_answer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"paragraph_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mindexes_top_N\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquestion_answer_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_question\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_N\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-1a152e5a9d20>\u001b[0m in \u001b[0;36mprocess_question\u001b[1;34m(question, vectorizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#A partir d'une question au format string et d'un vectorizer, renvoie un vecteur après pré-traitement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#et vectorisation de la question\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpreprocessed_question\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed_question\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-3e82f4048872>\u001b[0m in \u001b[0;36mtext_preprocessor\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtext_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mclean_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_small_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatize_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-3e82f4048872>\u001b[0m in \u001b[0;36mremove_stopwords\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-3e82f4048872>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     21\u001b[0m         return [\n\u001b[0;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         ]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, encoding)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m             \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, stream, encoding, errors)\u001b[0m\n\u001b[0;32m   1034\u001b[0m            beginning of ``linebuffer`` (which is required by ``tell()``).\"\"\"\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_bom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m         \"\"\"The length of the byte order marker at the beginning of\n\u001b[0;32m   1038\u001b[0m            the stream (or None for no byte order marker).\"\"\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_check_bom\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbom_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m             \u001b[1;31m# Read a prefix, to check against the BOM(s)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1403\u001b[1;33m             \u001b[0mbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1404\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Défini la taille de la figure\n",
    "fig = plt.figure(1, figsize=(10, 6))\n",
    "\n",
    "#Trace les deux courbes pour le modèle M1\n",
    "generate_plot(data_validation, data_corpus[0:index_to_cut], vectorizer_M1, euclidean_distances, tfidf_M1, tops_N, \"M1\", \"euclidienne\", question_answer)\n",
    "generate_plot(data_validation, data_corpus[0:index_to_cut], vectorizer_M1, cosine_distances, tfidf_M1, tops_N, \"M1\", \"cosinus\", question_answer)\n",
    "\n",
    "#Légende\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"Précision (%)\")\n",
    "plt.title(\"Précision top-N pour le modèle M1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17dc1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Défini la taille de la figure\n",
    "fig = plt.figure(1, figsize=(10, 6))\n",
    "\n",
    "#Trace les deux courbes pour le modèle M2\n",
    "generate_plot(data_questions[0:index_to_cut], data_corpus[0:index_to_cut], vectorizer_M2, euclidean_distances, tfidf_M2, tops_N, \"M2\", \"euclidienne\", question_answer)\n",
    "generate_plot(data_questions[0:index_to_cut], data_corpus[0:index_to_cut], vectorizer_M2, cosine_distances, tfidf_M2, tops_N, \"M2\", \"cosinus\", question_answer)\n",
    "\n",
    "#Légende\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"Précision (%)\")\n",
    "plt.title(\"Précision top-N pour le modèle M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83efed6-eba2-4d9c-9329-f49e94c601d6",
   "metadata": {},
   "source": [
    "3. (_5 points_) A cette étape, vous devez produire un fichier _passage_submission_M1.csv_ et _passage_submission_M2.csv_ qui contient pour toutes les questions de l’ensemble de test le top-N des passages retournés par votre modèle M1 et M2 pour y répondre. C’est à vous de déterminer si vous utiliserez la distance euclidienne ou cosinus basé sur vos résultats d’évaluation sur l’ensemble de validation en 1) et 2). Le fichier doit respecter le format suivant pour chaque top_N(N=1,5,10,50) :  <QuestionID, PassageID1 ;… ;PassageIDN>. Le format est démontré dans _sample_passage_submission.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621634b9-162e-43f5-9aac-7c32d411213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_file(data_questions, data_corpus, vectorizer, chosen_distance, tfidf, tops_N, model_name, question_answer_func, write = True) :\n",
    "    #A partir des datasets de questions/corpus, d'un modèle donnée avec une distance choisie, créer le fichier\n",
    "    #passage_submission\n",
    "    \n",
    "    #Création d'une dataframe vide au format demandé\n",
    "    df_columns = [\"id\"] + [\"top_\" + str(current_N) for current_N in tops_N]\n",
    "    output_df = pandas.DataFrame(columns = df_columns)\n",
    "    \n",
    "    max_N = max(tops_N)\n",
    "    for i in range(len(data_questions)) :\n",
    "        raw_question = data_questions[\"question\"][i]\n",
    "        id_question = data_questions[\"id\"][i]\n",
    "        element_to_add = [id_question]\n",
    "        tfidf_question = process_question(raw_question, vectorizer)\n",
    "        indexes_top_N = list(map(lambda x: str(x), question_answer_func(tfidf_question, tfidf, data_corpus, chosen_distance, max_N).index.to_list()))\n",
    "        \n",
    "        #Plutôt que de recalculer les top-N réponses à chaque fois, on calcule sur le plus grand top-N et on split la liste\n",
    "        for current_N in tops_N :\n",
    "            elm = \";\".join(indexes_top_N[0:current_N])\n",
    "            element_to_add.append(elm)\n",
    "            \n",
    "        output_df = output_df.append(pandas.DataFrame([element_to_add], columns = df_columns))\n",
    "        \n",
    "    output_df = output_df.set_index(\"id\")\n",
    "    if write :\n",
    "        output_df.to_csv(\"passage_submission_\" + model_name + \".csv\")\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création du fichier de passage pour M1\n",
    "output_df_M1 = generate_submission_file(data_questions[0:index_to_cut], data_corpus[0:index_to_cut], vectorizer_M1, euclidean_distances, tfidf_M1, tops_N, \"M1\", question_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création du fichier de passage pour M2\n",
    "output_df_M2 = generate_submission_file(data_questions[0:index_to_cut], data_corpus[0:index_to_cut], vectorizer_M2, euclidean_distances, tfidf_M2, tops_N, \"M2\", question_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5443fae-3acb-442d-8086-18dfa744e832",
   "metadata": {},
   "source": [
    "### 5.5. Le plus (24 points)\n",
    "\n",
    "1. (_21 points_) Vous devez proposer un modèle M3 différent (basé sur l’apprentissage machine par exemple) afin de déterminer un score de pertinence d’un passage pour une question donnée et ordonner les passages. \n",
    "    - Faites une petite recherche sur l’état de l’art en consultant https://nlp.stanford.edu/IR-book/information-retrieval-book.html\n",
    "    - Vous êtes libres de proposer une autre métrique de poids, ou une autre façon d’ordonner les passages (exemple : méthodes de type _learning to rank_) et de partir de votre corpus initial ou de votre ordonnancement en M1/M2 (choisissez le meilleur) et de réordonnancer les passages obtenus par votre premier modèle.\n",
    "    - Expliquez votre modèle et son intérêt dans votre notebook. Le nombre de points obtenus dépendra de l’effort mis dans cette partie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train = vectorizer_M1.transform(processed_questions[\"processed\"]).toarray()\n",
    "y_train = data_questions[0:index_to_cut][\"paragraph_id\"]\n",
    "X_valid = vectorizer_M1.transform(preprocessing(data_validation[0:10], \"question\")[\"processed\"]).toarray()\n",
    "y_valid = data_validation[\"paragraph_id\"][0:10]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_model = DecisionTreeClassifier(max_depth = 20).fit(X_train, y_train)\n",
    "dtree_predictions = dtree_model.predict(X_valid)\n",
    "dtree_predictions\n",
    "\n",
    "#from sklearn.svm import SVC\n",
    "#svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "#svm_predictions = svm_model_linear.predict(X_valid)\n",
    "\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train)\n",
    "#knn_predictions = knn.predict(X_valid)\n",
    "\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#gnb = GaussianNB().fit(X_train, y_train)\n",
    "#gnb_predictions = gnb.predict(X_valid)\n",
    "\n",
    "print(y_valid)\n",
    "dtree_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Autre modèle ci-dessous, pas sur que ça run\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8556f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_it_the_good_answer(answer_id, right_id) :\n",
    "    if answer_id == right_id :\n",
    "        return 1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataframe(data_questions, data_corpus, vectorizer, chosen_distance, tfidf, top_N) :\n",
    "    list_of_X = []\n",
    "    list_of_Y = []\n",
    "    for i in range(len(data_questions)) :\n",
    "        raw_question = data_questions[\"question\"][i]\n",
    "        tfidf_question = process_question(raw_question, vectorizer)\n",
    "        paragraph_id_answer = data_questions[\"paragraph_id\"][i]\n",
    "        answers = question_answer(tfidf_question, tfidf, data_corpus, chosen_distance, top_N)\n",
    "        Yi = pandas.DataFrame(answers[\"id\"].apply(lambda x: int(x==paragraph_id_answer)))\n",
    "        Xi = pandas.DataFrame(answers[\"distance\"])\n",
    "        list_of_X.append(Xi)\n",
    "        list_of_Y.append(Yi)\n",
    "    X = pandas.concat(list_of_X)\n",
    "    Y = pandas.concat(list_of_Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d67030",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_training_dataframe(data_questions, data_corpus[0:index_to_cut], vectorizer_M1, euclidean_distances, tfidf_M1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ba1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RandomForestClassifier\n",
    "\n",
    "def train_model(X, Y) :\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X,Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest_model = train_model(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58eeac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer_M3(tfidf_question, tfidf_corpus, data_corpus, distance, top_N, model) : \n",
    "    answers = question_answer(tfidf_question, tfidf_corpus, data_corpus, distance, top_N*2)\n",
    "    ypred = randomforest_model.predict(answers[\"distance\"])\n",
    "    print(\"Dataframe predicted :\", ypred)\n",
    "    ypred.columns = ['prediction']\n",
    "    concated_df = pandas.concat([answers, ypred], axis=0, ignore_index=True)\n",
    "    print(\"Concated dataframe :\", concated_df)\n",
    "    return data_corpus_copy.sort_values(by='prediction', ascending=False)[:top_N]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f85d57-176a-4276-878d-2d9d5b5aea9b",
   "metadata": {},
   "source": [
    "2. (_2  point_) Vous devez ensuite afficher l’évaluation de votre modèle M3 tel que décrit dans la section 5.4 Evaluation en utilisant les mêmes fonctions. Notamment, vous devez comparer les performances de vos modèles M1, M2 et M3 sur l’ensemble de validation avec une courbe de performance faisant varier le N (N=1, 5, 10, …)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ee33d-a842-490c-ad8e-ab8c8ea3f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Défini la taille de la figure\n",
    "fig = plt.figure(1, figsize=(10, 6))\n",
    "\n",
    "#Trace les deux courbes pour le modèle M3\n",
    "generate_plot(data_validation, data_corpus[0:index_to_cut], vectorizer_M1, euclidean_distances, tfidf_M1, tops_N, \"M1\", \"euclidienne\", question_answer_M3)\n",
    "generate_plot(data_validation, data_corpus[0:index_to_cut], vectorizer_M1, cosine_distances, tfidf_M1, tops_N, \"M1\", \"cosinus\", question_answer_M3)\n",
    "\n",
    "#Légende\n",
    "plt.xlabel(\"N\")\n",
    "plt.ylabel(\"Précision (%)\")\n",
    "plt.title(\"Précision top-N pour le modèle M1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070749a9-c234-4340-a78c-ce63195c9532",
   "metadata": {},
   "source": [
    "3. (_1 point_) En utilisant votre modèle M3, vous devez produire un fichier passage_submission_M3.csv qui contient pour toutes les questions de l’ensemble de test le top-N des passages retournés par votre système pour y répondre. Le fichier doit respecter le format suivant pour chaque top_N (N=1,5,10,50) :  <QuestionID, PassageID1…PassageIDN>. _Le format est démontré dans sample_passage_submission.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7657f0-5336-462e-af3c-9ad4e5c30bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df_M3 = generate_submission_file(data_questions[0:index_to_cut], data_corpus[0:index_to_cut], vectorizer_M1, euclidean_distances, tfidf_M1, tops_N, \"M1\", question_answer_M3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1735914-0bff-48b8-a160-5f56a8766c31",
   "metadata": {},
   "source": [
    "## LIVRABLES\n",
    "Vous devez remettre sur Moodle:\n",
    "1. _Le code_ : Un Jupyter notebook en Python qui contient le code implanté avec les librairies permises. Le code doit être exécutable sans erreur et accompagné des commentaires appropriés dans le notebook de manière à expliquer les différentes fonctions et étapes dans votre projet. Nous nous réservons le droit de demander une démonstration ou la preuve que vous avez effectué vous-mêmes les expériences décrites. _Attention, en aucun cas votre code ne doit avoir été copié d’une quelconque source_. Les critères de qualité tels que la lisibilité du code et des commentaires sont importants. Tout votre code et vos résultats doivent être exécutables et reproductibles ; \n",
    "2. Un fichier _requirements.txt_ doit indiquer toutes les librairies / données nécessaires ;\n",
    "3. Un lien _GoogleDrive_ ou similaire vers les modèles nécessaires pour exécuter votre notebook si approprié ;\n",
    "4. Les fichiers de soumission de données de test _passage_submission_M1.csv_ et _passage_submission_M2.csv_\n",
    "5. Un document _contributions.txt_ : Décrivez brièvement la contribution de chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail. En particulier, tous les membres du projet devraient participer à la conception du TP et participer activement à la réflexion et à l’implémentation du code.\n",
    "\n",
    "## EVALUATION \n",
    "Votre TP sera évalué selon les critères suivants :\n",
    "1. Exécution correcte du code\n",
    "2. Performance correcte des modèles\n",
    "3. Organisation du notebook\n",
    "4. Qualité du code (noms significatifs, structure, performance, gestion d’exception, etc.)\n",
    "5. Commentaires clairs et informatifs\n",
    "\n",
    "## CODE D’HONNEUR\n",
    "- Règle 1:  Le plagiat de code est bien évidemment interdit.\n",
    "- Règle 2: Vous êtes libres de discuter des idées et des détails de mise en œuvre avec d'autres équipes. Cependant, vous ne pouvez en aucun cas consulter le code d'une autre équipe INF8460, ou incorporer leur code dans votre TP.\n",
    "- Règle 3:  Vous ne pouvez pas partager votre code publiquement (par exemple, dans un dépôt GitHub public) tant que le cours n'est pas fini.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
